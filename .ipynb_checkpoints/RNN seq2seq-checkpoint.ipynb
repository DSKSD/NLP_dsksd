{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq translate 분해 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 패키지 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Sequence-to-sequence model with an attention mechanism.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.models.rnn.translate import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2SeqModel(object):\n",
    "  \"\"\"Sequence-to-sequence model with attention and for multiple buckets.\n",
    "  This class implements a multi-layer recurrent neural network as encoder,\n",
    "  and an attention-based decoder. This is the same as the model described in\n",
    "  this paper: http://arxiv.org/abs/1412.7449 - please look there for details,\n",
    "  or into the seq2seq library for complete model implementation.\n",
    "  This class also allows to use GRU cells in addition to LSTM cells, and\n",
    "  sampled softmax to handle large output vocabulary size. A single-layer\n",
    "  version of this model, but with bi-directional encoder, was presented in\n",
    "    http://arxiv.org/abs/1409.0473\n",
    "  and sampled softmax is described in Section 3 of the following paper.\n",
    "    http://arxiv.org/abs/1412.2007\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               source_vocab_size,\n",
    "               target_vocab_size,\n",
    "               buckets,\n",
    "               size,\n",
    "               num_layers,\n",
    "               max_gradient_norm,\n",
    "               batch_size,\n",
    "               learning_rate,\n",
    "               learning_rate_decay_factor,\n",
    "               use_lstm=False,\n",
    "               num_samples=512,\n",
    "               forward_only=False,\n",
    "               dtype=tf.float32):\n",
    "    \"\"\"Create the model.\n",
    "    Args:\n",
    "      source_vocab_size: 소스 어휘의 사이즈(영어)\n",
    "      target_vocab_size: 타겟 어휘의 사이즈(프랑스)\n",
    "      buckets:버킷의 사이즈<소팅된 리스트> [(2, 4), (8, 16)].\n",
    "      size: 각 레이어 유닛의 수\n",
    "      num_layers: 레이어의 수\n",
    "      max_gradient_norm: gradients will be clipped to maximally this norm.\n",
    "      batch_size: the size of the batches used during training;\n",
    "        the model construction is independent of batch_size, so it can be\n",
    "        changed after initialization if this is convenient, e.g., for decoding.\n",
    "      learning_rate: learning rate to start with.\n",
    "      learning_rate_decay_factor: decay learning rate by this much when needed.\n",
    "      use_lstm: if true, we use LSTM cells instead of GRU cells.\n",
    "      num_samples: number of samples for sampled softmax.\n",
    "      forward_only: if set, we do not construct the backward pass in the model. # 테스트 or predict 할 때\n",
    "      dtype: the data type to use to store internal variables.\n",
    "    \"\"\"\n",
    "    self.source_vocab_size = source_vocab_size\n",
    "    self.target_vocab_size = target_vocab_size\n",
    "    self.buckets = buckets\n",
    "    self.batch_size = batch_size\n",
    "    self.learning_rate = tf.Variable(\n",
    "        float(learning_rate), trainable=False, dtype=dtype)\n",
    "    self.learning_rate_decay_op = self.learning_rate.assign(\n",
    "        self.learning_rate * learning_rate_decay_factor)\n",
    "    self.global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # buckets = [(5,10), (10,15), (20,25), (40,50)]\n",
    "    # 모델 초기화!!\n",
    "    \n",
    "    # sampled softmax를 사용하려면 output_projection이 필요하다\n",
    "    output_projection = None\n",
    "    softmax_loss_function = None\n",
    "    # Sampled softmax only makes sense if we sample less than vocabulary size.\n",
    "    # 샘플의 갯수가 target_vocab_size보다 작은 경우에만 사용한다.\n",
    "    # 더 큰 경우에는 표준 소프트맥스 손실함수가 낫다.\n",
    "    \n",
    "    if num_samples > 0 and num_samples < self.target_vocab_size:\n",
    "      w_t = tf.get_variable(\"proj_w\", [self.target_vocab_size, size], dtype=dtype)\n",
    "      w = tf.transpose(w_t)\n",
    "      b = tf.get_variable(\"proj_b\", [self.target_vocab_size], dtype=dtype)\n",
    "      output_projection = (w, b)\n",
    "    \n",
    "    \"\"\"\n",
    "    weight matrix와 bias vector의 쌍으로 이루어진 출력 투영을 구성한다.\n",
    "    이것이 사용되는 경우, rnn cell은 (batch size x target_vocab_size)\n",
    "    대신 (batch size x size) 형태의 벡터를 반환한다.\n",
    "    로짓을 복원하려면 가중치 행렬로 곱한 뒤 편향 벡터를 더해야 한다. \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "      def sampled_loss(inputs, labels):\n",
    "        labels = tf.reshape(labels, [-1, 1])\n",
    "        # We need to compute the sampled_softmax_loss using 32bit floats to\n",
    "        # avoid numerical instabilities.\n",
    "        local_w_t = tf.cast(w_t, tf.float32)\n",
    "        local_b = tf.cast(b, tf.float32)\n",
    "        local_inputs = tf.cast(inputs, tf.float32)\n",
    "        return tf.cast(\n",
    "            tf.nn.sampled_softmax_loss(local_w_t, local_b, local_inputs, labels,\n",
    "                                       num_samples, self.target_vocab_size),\n",
    "            dtype)\n",
    "      softmax_loss_function = sampled_loss\n",
    "\n",
    "    # Create the internal multi-layer cell for our RNN.\n",
    "    single_cell = tf.nn.rnn_cell.GRUCell(size)\n",
    "    if use_lstm:\n",
    "      single_cell = tf.nn.rnn_cell.BasicLSTMCell(size)\n",
    "    cell = single_cell\n",
    "    if num_layers > 1:\n",
    "      cell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * num_layers)\n",
    "\n",
    "    # The seq2seq function: we use embedding for the input and attention.\n",
    "    def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n",
    "      return tf.nn.seq2seq.embedding_attention_seq2seq(\n",
    "          encoder_inputs,\n",
    "          decoder_inputs,\n",
    "          cell,\n",
    "          num_encoder_symbols=source_vocab_size,\n",
    "          num_decoder_symbols=target_vocab_size,\n",
    "          embedding_size=size,\n",
    "          output_projection=output_projection,\n",
    "          feed_previous=do_decode,\n",
    "          dtype=dtype)\n",
    "    \"\"\"\n",
    "    encoder_inputs, decoder_inputs은 이산값을 나타내는 정수 벡터\n",
    "    이 입력은 밀도 높은 표현으로 임베드되는데 이 임베딩을 구성하기 위해\n",
    "    심볼들의 숫자들(num_encoder_symbols, num_decoder_symbols)을 지정할\n",
    "    필요가 있다.\n",
    "    \n",
    "    * feed_previous : False이면 decoder_inputs 텐서를 그대로 사용,\n",
    "    True면 decoder_inputs의 첫 번째 원소만 사용한다? 리스트의 다른 텐서는\n",
    "    모두 무시되고, 인코더의 이전 결과를 대신 사용\n",
    "    \n",
    "    * output_projection : 이 인자가 지정되지 않으면, 임베딩 모델의 출력은\n",
    "    생성된 각 심볼의 로짓을 나타내는 (batch size x num_decoder_symbols)\n",
    "    형태의 텐서로 나오게 되는데, 이렇게 되면 (num_decoder_symbols)가 클 때\n",
    "    실용적이지 않게 된다. \n",
    "    \n",
    "    먼저 작은 출력 텐서를 반환하고 이후에 output_projection을 이용해서 큰 출력\n",
    "    텐서로 projection하는 편이 낫다. 이렇게 해서 sampled softmax loss와 함께\n",
    "    사용할 수 있다.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # bucket과 padding 사용\n",
    "    \n",
    "    # Feeds for inputs.\n",
    "    self.encoder_inputs = []\n",
    "    self.decoder_inputs = []\n",
    "    self.target_weights = []\n",
    "    for i in xrange(buckets[-1][0]):  # Last bucket is the biggest one.\n",
    "      self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n",
    "                                                name=\"encoder{0}\".format(i)))\n",
    "    for i in xrange(buckets[-1][1] + 1):\n",
    "      self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n",
    "                                                name=\"decoder{0}\".format(i)))\n",
    "      self.target_weights.append(tf.placeholder(dtype, shape=[None],\n",
    "                                                name=\"weight{0}\".format(i)))\n",
    "\n",
    "    # Our targets are decoder inputs shifted by one.\n",
    "    targets = [self.decoder_inputs[i + 1]\n",
    "               for i in xrange(len(self.decoder_inputs) - 1)]\n",
    "\n",
    "    # Training outputs and losses.\n",
    "    if forward_only:\n",
    "      self.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(\n",
    "          self.encoder_inputs, self.decoder_inputs, targets,\n",
    "          self.target_weights, buckets, lambda x, y: seq2seq_f(x, y, True),\n",
    "          softmax_loss_function=softmax_loss_function)\n",
    "      # If we use output projection, we need to project outputs for decoding.\n",
    "      if output_projection is not None: # 여기서 로짓을 복원?\n",
    "        for b in xrange(len(buckets)):\n",
    "          self.outputs[b] = [\n",
    "              tf.matmul(output, output_projection[0]) + output_projection[1]\n",
    "              for output in self.outputs[b]\n",
    "          ]\n",
    "    else:\n",
    "      self.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(\n",
    "          self.encoder_inputs, self.decoder_inputs, targets,\n",
    "          self.target_weights, buckets,\n",
    "          lambda x, y: seq2seq_f(x, y, False),\n",
    "          softmax_loss_function=softmax_loss_function)\n",
    "\n",
    "    # Gradients and SGD update operation for training the model.\n",
    "    params = tf.trainable_variables()\n",
    "    if not forward_only:\n",
    "      self.gradient_norms = []\n",
    "      self.updates = []\n",
    "      opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "      for b in xrange(len(buckets)):\n",
    "        gradients = tf.gradients(self.losses[b], params)\n",
    "        clipped_gradients, norm = tf.clip_by_global_norm(gradients,\n",
    "                                                         max_gradient_norm)\n",
    "        self.gradient_norms.append(norm)\n",
    "        self.updates.append(opt.apply_gradients(\n",
    "            zip(clipped_gradients, params), global_step=self.global_step))\n",
    "\n",
    "    self.saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "  def step(self, session, encoder_inputs, decoder_inputs, target_weights,\n",
    "           bucket_id, forward_only):\n",
    "    \"\"\"Run a step of the model feeding the given inputs.\n",
    "    Args:\n",
    "      session: tensorflow session to use.\n",
    "      encoder_inputs: list of numpy int vectors to feed as encoder inputs.\n",
    "      decoder_inputs: list of numpy int vectors to feed as decoder inputs.\n",
    "      target_weights: list of numpy float vectors to feed as target weights.\n",
    "      bucket_id: which bucket of the model to use.\n",
    "      forward_only: whether to do the backward step or only forward.\n",
    "    Returns:\n",
    "      A triple consisting of gradient norm (or None if we did not do backward),\n",
    "      average perplexity, and the outputs.\n",
    "    Raises:\n",
    "      ValueError: if length of encoder_inputs, decoder_inputs, or\n",
    "        target_weights disagrees with bucket size for the specified bucket_id.\n",
    "    \"\"\"\n",
    "    # Check if the sizes match.\n",
    "    encoder_size, decoder_size = self.buckets[bucket_id]\n",
    "    if len(encoder_inputs) != encoder_size:\n",
    "      raise ValueError(\"Encoder length must be equal to the one in bucket,\"\n",
    "                       \" %d != %d.\" % (len(encoder_inputs), encoder_size))\n",
    "    if len(decoder_inputs) != decoder_size:\n",
    "      raise ValueError(\"Decoder length must be equal to the one in bucket,\"\n",
    "                       \" %d != %d.\" % (len(decoder_inputs), decoder_size))\n",
    "    if len(target_weights) != decoder_size:\n",
    "      raise ValueError(\"Weights length must be equal to the one in bucket,\"\n",
    "                       \" %d != %d.\" % (len(target_weights), decoder_size))\n",
    "\n",
    "    # Input feed: encoder inputs, decoder inputs, target_weights, as provided.\n",
    "    input_feed = {}\n",
    "    for l in xrange(encoder_size):\n",
    "      input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n",
    "    for l in xrange(decoder_size):\n",
    "      input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n",
    "      input_feed[self.target_weights[l].name] = target_weights[l]\n",
    "\n",
    "    # Since our targets are decoder inputs shifted by one, we need one more.\n",
    "    last_target = self.decoder_inputs[decoder_size].name\n",
    "    input_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\n",
    "\n",
    "    # Output feed: depends on whether we do a backward step or not.\n",
    "    if not forward_only:\n",
    "      output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\n",
    "                     self.gradient_norms[bucket_id],  # Gradient norm.\n",
    "                     self.losses[bucket_id]]  # Loss for this batch.\n",
    "    else:\n",
    "      output_feed = [self.losses[bucket_id]]  # Loss for this batch.\n",
    "      for l in xrange(decoder_size):  # Output logits.\n",
    "        output_feed.append(self.outputs[bucket_id][l])\n",
    "\n",
    "    outputs = session.run(output_feed, input_feed)\n",
    "    if not forward_only:\n",
    "      return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\n",
    "    else:\n",
    "      return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\n",
    "\n",
    "  def get_batch(self, data, bucket_id):\n",
    "    \"\"\"Get a random batch of data from the specified bucket, prepare for step.\n",
    "    To feed data in step(..) it must be a list of batch-major vectors, while\n",
    "    data here contains single length-major cases. So the main logic of this\n",
    "    function is to re-index data cases to be in the proper format for feeding.\n",
    "    Args:\n",
    "      data: a tuple of size len(self.buckets) in which each element contains\n",
    "        lists of pairs of input and output data that we use to create a batch.\n",
    "      bucket_id: integer, which bucket to get the batch for.\n",
    "    Returns:\n",
    "      The triple (encoder_inputs, decoder_inputs, target_weights) for\n",
    "      the constructed batch that has the proper format to call step(...) later.\n",
    "    \"\"\"\n",
    "    encoder_size, decoder_size = self.buckets[bucket_id]\n",
    "    encoder_inputs, decoder_inputs = [], []\n",
    "\n",
    "    # Get a random batch of encoder and decoder inputs from data,\n",
    "    # pad them if needed, reverse encoder inputs and add GO to decoder.\n",
    "    for _ in xrange(self.batch_size):\n",
    "      encoder_input, decoder_input = random.choice(data[bucket_id])\n",
    "\n",
    "      # Encoder inputs are padded and then reversed.\n",
    "      encoder_pad = [data_utils.PAD_ID] * (encoder_size - len(encoder_input))\n",
    "      encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\n",
    "\n",
    "      # Decoder inputs get an extra \"GO\" symbol, and are padded then.\n",
    "      decoder_pad_size = decoder_size - len(decoder_input) - 1\n",
    "      decoder_inputs.append([data_utils.GO_ID] + decoder_input +\n",
    "                            [data_utils.PAD_ID] * decoder_pad_size)\n",
    "\n",
    "    # Now we create batch-major vectors from the data selected above.\n",
    "    batch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\n",
    "\n",
    "    # Batch encoder inputs are just re-indexed encoder_inputs.\n",
    "    for length_idx in xrange(encoder_size):\n",
    "      batch_encoder_inputs.append(\n",
    "          np.array([encoder_inputs[batch_idx][length_idx]\n",
    "                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n",
    "\n",
    "    # Batch decoder inputs are re-indexed decoder_inputs, we create weights.\n",
    "    for length_idx in xrange(decoder_size):\n",
    "      batch_decoder_inputs.append(\n",
    "          np.array([decoder_inputs[batch_idx][length_idx]\n",
    "                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n",
    "\n",
    "      # Create target_weights to be 0 for targets that are padding.\n",
    "      batch_weight = np.ones(self.batch_size, dtype=np.float32)\n",
    "      for batch_idx in xrange(self.batch_size):\n",
    "        # We set weight to 0 if the corresponding target is a PAD symbol.\n",
    "        # The corresponding target is decoder_input shifted by 1 forward.\n",
    "        if length_idx < decoder_size - 1:\n",
    "          target = decoder_inputs[batch_idx][length_idx + 1]\n",
    "        if length_idx == decoder_size - 1 or target == data_utils.PAD_ID:\n",
    "          batch_weight[batch_idx] = 0.0\n",
    "      batch_weights.append(batch_weight)\n",
    "    return batch_encoder_inputs, batch_decoder_inputs, batch_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
