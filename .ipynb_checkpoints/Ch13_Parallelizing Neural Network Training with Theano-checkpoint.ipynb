{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch13. Parallelizing Neural Network Training with Theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 발표자 : 정지원**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   이전 장에서 어떻게 인공신경망과 멀티레이어 퍼셉트론이 feedforward 하는지 수학적인 개념에 대해 다뤘다. 머신 러닝 알고리즘의 수학적인 기반을 먼저 이해하는 것은 매우 중요하다. 왜냐하면 이런 이해를 통해 매우 효과적이고 올바르게 알고리즘을 사용할 수 있다. 이전 챕터들에서 밑바닥부터 머신러닝의 best practice와 알고리즘 구현을 배웠다. 이번 장에서는 편안한 마음으로 머신러닝 연구원들이 딥뉴럴넷의 실험과 학습에 매우 유용하게 사용하는 강력한 라이브러리에 대해 살펴보겠다. 최근 머신러닝 연구는 **GPU**를 사용한다. 딥러닝에 관심이 있다면, 이번 챕터는 당신의 것이다. GPU사용은 옵션이니 걱정마라."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 다음 내용을 배운다.\n",
    "\n",
    "    • Writing optimized machine learning code with Theano\n",
    "    \n",
    "    • Choosing activation functions for artificial neural networks\n",
    "    \n",
    "    • Using the Keras deep learning library for fast and easy experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building, compiling, and running expressions with Theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 섹션에서는, 파이썬을 통해 매우 효과적으로 머신러닝 모델 학습을 해주는 강력한 **Theano tool**에 대해 배운다.\n",
    "\n",
    "The Theano development started back in 2008 in the LISA lab (short for Laboratoire d'Informatique des Systèmes Adaptatifs (http://lisa.iro.umontreal.ca)) lead by Yoshua Bengio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theano에 대해 배우기 전에, 우리가 머신러 속도를 높이기 위해 무엇을 할지 살펴보자. 우리가 매우 많은 연산을 할 때의 문제에 대해 얘기해보자. 운좋게도 컴퓨터 프로세서의 성능은 해가갈수록 꾸준히 좋아지고 있다. 이 때문에 우리가 머신러닝 모델의 예측 성능을 향상시키기 위한 강력하고 복잡한 학습 시스템을 만들 수 있다. 이전 챕터에서, 싸이킷-런의 많은 함수들이 다수의 프로세싱 유닛에 계산을 분산시켜줬다. 그러나 기본적으로 파이썬은 ** Global Interpreter Lock** 때문에 one core의 실행에 제한돼있다. 하지만 비록 우리가 _multiprocessing_ 라이브러리의 이점이 있더라도, 좋은 데스크탑의 경우도 8/16 이상의 코어를 갖는게 거의 없다는 사실을 고려해야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전 챕터로 돌아가서, 히든 레이어를 하나 갖는 50개 units의 아주 단순한 멀티레이어 퍼셉트론을 생각해보자. 매우 단순한 이미지 분류 문제를 위해 대략 1000 weights를 최적화시켜야 했다. **MNIST**의 이미지는 다소 작았지만, 만약에 히든레이어가 하나 추가되고 이미지가 높은 픽셀밀도를 가졌다고 상상해보자. 이러한 작업은 single processing unit으로는 실행할 수 없게 될 것이다. 이제 문제는 이러한 문제를 어떻게 효과적으로 다룰지이다. 좋은 방법으로는 GPU를 사용하는 것이다. GPU는 real power horses다. 그래픽 카드를 당신 기계 안의 작은 컴퓨터 클러스터로 보면 된다. 최근 GPU의 장점은 최신 CPU와 비교했을때 상대적으로 저렴한 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](figures/1.png)\n",
    "\n",
    "(date : August 20, 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU의 70% 가격으로 GPU에서 450배의 코어를 얻을 수 있다. 초당 15배의 부동소수점 연산이 가능하다. 이제 문제는, GPU을 target하기 위한 코드를 적는 것은, 파이썬 코드를 실행하는 것과 같이 하찮은 것이 아니다. __CUDA__와 __OpenCL__과 같은 특별한 패키지들은 GPU 타겟하는 것을 도와줄것이다. 그러나 __CUDA__나 __OpenCL__와 같은 코드를 입력하는 것은 아마 머신러닝 알고리즘을 실행하는데 가장 좋은 환경은 아닐 것이다. 더 좋은 소식은 __Theano__가 개발된 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Theano?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정확히 Theano가 뭐냐? 컴퓨터 언어? 컴파일러? 파이썬 라이브러리?\n",
    "\n",
    "이 모든 표현이 말이 된다. Theano는 다중차원의 array에 강력하게 초점을 맞춰 실행하고, 컴파일하고, 수학적 표현을 계산하는데 매우 효과적이다. CPU로 코드를 돌리는 옵션도 있다. 그러나 GPU를 사용하는 것이 큰 메모리 대역폭의 장점과 뛰어난 부동소수점 계산의 능력의 이점을 얻을 것이다. Theano를 이용하여 메모리 공유를 동시에 하는 코드를 쉽게 실행할 수 있다. 2010년도에, CPU 사용에서 Theano는 NumPy보다 1.8배 빠른 성과를 보였다. GPU에서는 11배 빠른 속도를 보였다. Theano의 성능과 그래픽카드의 성능은 상당히 개선되어왔다.\n",
    "\n",
    "NumPy가 Theano랑 무슨 관련이 있는가? Theano는 NumPy 기반으로 만들어졌다. 그리고 매우 유사한 구문을 사용하며, 이런 언어에 친숙한 사람들에게 편리성을 준다. Theano는 많은 사람들이 언급하는 \"NumPy on steroids\" 뿐만 아니라, SymPy와의 몇몇 유사성을 공유하기도 한다. SymPy는 symbolic computation(or symbolic algebra)을 제공하는 파이썬 패키지다. 이전장에서 봤듯이, NumPy에서 변수를 정의하고 원하는 방식으로 조합했다. 그리고 코드는 줄줄이 실행됐다. 하지만 Theano에서는, 문제를 먼저 적고, 우리가 어떻게 분석하기를 원하는지 적는다. 그러면 Theano가 코드를 컴파일 하고 최적화 시켜준다. 최적화된 코드를 작성하기 위해서 Theano는 우리 문제의 범위를 알아야만 한다. 이것을 tree of operations라고 생각해봐라.(혹은 상징적 표현의 그래프) Theano는 여전히 개발되는 중으로 많은 기능들이 추가되었고, 이러한 발전은 정기적으로 이루어진다. 이번 챕터에서, 우리는 Theano의 기본 개념에 대해 알아보고, 어떻게 머신러닝을 수행하는데 사용되는지 배울 것이다. Theano는 많은 고급기능을 가진 라이브러리기 때문에, 다 다루기는 힘들다. 다음 링크에서 더 배울 수 있다.\n",
    "\n",
    "(http://deeplearning.net/software/theano/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First steps with Theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "설치 방법,\n",
    "\n",
    "__pip install Theano__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theano는 core에 상징화된 수식 표현을 계산하기 위한 tensor를 내장하고 있다. 텐서는 스칼라, 벡터, 행렬 등으로 이해될 수 있다. 구체적으로, 스칼라는 rank-0 텐서이며, 벡터는 rank-1 텐서, 행렬은 rank-2 텐서이다. 그리고 3차원으로 만들어진 행렬들은 rank-3 텐서이다. 간단한 스칼라를 사용하기 위해 _tensor_ 모듈을 사용해보자. z를 계산하기 위해, sample point x와 weight w1과 bias w0을 구해보자. \n",
    "\n",
    "![Image](figures/2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net input: 2.50\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "from theano import tensor as T\n",
    "# initialize\n",
    "x1 = T.scalar()\n",
    "w1 = T.scalar()\n",
    "w0 = T.scalar()\n",
    "z1 = w1 * x1 + w0\n",
    "# compile\n",
    "net_input = theano.function(inputs=[w1, x1, w0],outputs=z1)\n",
    "# execute\n",
    "print('Net input: %.2f' % net_input(2.0, 1.0, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이것은 매우 직관적이다. Theano에서 코드를 작성한다면, 다음 세 가지 간단한 과정을 따르기만 하면 된다. 1)_symbols_를 정의하고, 2)code를 컴파일하고, 3)실행한다. 시작 단계에서, z를 구하기 위해 x1, w1, w0 세 가지 심볼을 정의했다. 그리고 net input z1을 계산하기 위해 _net_input_ 함수를 컴파일했다.\n",
    "\n",
    "그러나 Theano 코드를 작성할 때, 한 가지 특별히 주의해야 할 것이 있다. 변수의 타입이다. 좋아할 일이든 아니든간에 64/32 bit integer나 float 중에 어떤 것을 사용하고 싶은지 선택해야 한다. 이것은 코드의 성능에 매우 영향을 주기 때문에 다음 섹션에서 더 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring Theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "요즘, 맥OS든 리눅스든 윈도우든 상관 없이, 소프트웨어나 어플리케이션들은 주로 64bit 메모리 주소를 이용한다. 그러나 우리가 GPU에서 수학적 표현을 계산을 가속화하기 위해서는, 우리는 32bit 메모리주소에 의존해야만 한다. 최근에는, Theano에서 오직 이 컴퓨팅 구조만을 지원해준다. 이번 섹션에서, Theano를 어떻게 구성하는지 볼 것이다. 만약 더 세부적으로 알고 싶다면, 다음을 참고해라.\n",
    "(http://deeplearning.net/software/theano/library/config.html. )\n",
    "\n",
    "머신러닝 알고리즘을 구현할 때, 대부분 부동소수점수를 이용한다. 기본적으로, NumPy와 Theano는 _double-precision flaoting-point numbers_(float64)를 사용한다. 하지만 CPU에서 코드를 짜고 GPU에서 실행 시킬때는 float64와 float32를 앞뒤로 전환하는 것이 유용할 것이다. 예를 들어, Theano의 flaot 변수를 기본적으로 설정하기 위해 다음과 같은 코드를 입력하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "print(theano.config.floatX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "셋팅을 바꾸고 싶으면, 다음과 같이 입력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theano.config.floatX = 'float32'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theano에서 GPU를 이용할 때, float32를 필요로 하지만, CPU에서 float64와 float32 모두 사용 가능하다. 그러므로 Command-line terminal를 통해 다음과 같이 _THEANO_FLAGS_의 세팅을 바꿀 수 있다.\n",
    "\n",
    "*export THEANO_FLAGS=floatX=float32*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "혹은 특정 파이썬 스크립트에만 적용하고자 한다면 다음과 같이 입력하면 된다.\n",
    "\n",
    "*THEANO_FLAGS=floatX=float32 python your_script.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU와 GPU 실행 사이의 전환하는 옵션에 대해 알아보자. 다음 코드를 통해 확인부터 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(theano.config.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "개인적인 추천으로는, 코드를 만들고 디버깅하기 쉽기 때문에 _cpu_를 기본으로 사용해라. 만약 command-line terminal에서 CPU로 실행하고 싶다면 다음과 같이 실행해라.\n",
    "\n",
    "*THEANO_FLAGS=device=cpu,floatX=float64 python your_script.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 일단 코드를 구현하고 GPU를 사용해 효율적으로 돌아가게 하고싶다면, 우리 원래 코드를 추가적으로 수정하지 않고도 다음 코드를 통해 실행시킬 수 있다.\n",
    "\n",
    "*THEANO_FLAGS=device=gpu,floatX=float32 python your_script.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_.theanorc_ 파일을 home directory에 만들어서 영구적으로 이러한 구성을 할 수도 있다. 예를 들어, 항상 float32와 GPU를 사용하고자 한다면 다음과 같은 _.theanorc_ 파일을 세팅해라. 명령어는 다음과 같다.\n",
    "\n",
    "*echo -e \"\\n[global]\\nfloatX=float32\\ndevice=gpu\\n\" >> ~/.theanorc*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "맥이나 리눅스를 사용하지 않는다면, 좋아하는 텍스트 에디터를 통해 다음과 같이 입력하여 수동으로 _.theanorc_ 파일을 만든다.\n",
    "\n",
    "_[global]\n",
    "floatX=float32\n",
    "device=gpu_\n",
    "\n",
    "이제 대충 어떻게 하드웨어에 대해 Theano를 구성하는지 알았다. 이제 더 복잡한 array 구조를 보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------\n",
    "\n",
    "## Working with array structures\n",
    "\n",
    "이번 섹션에서는, _tensor_모듈을 이용하여 Theano의 array 구조를 사용하는 방법을 배운다. 다음 코드를 실행하면, 2x3 행렬을 얻을 수 있다. Theano의 최적화된 텐서 표현을 통해 열의 합을 계산해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column sum: [ 2.  4.  6.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# initialize\n",
    "x = T.fmatrix(name='x')\n",
    "x_sum = T.sum(x, axis=0)\n",
    "# compile\n",
    "calc_sum = theano.function(inputs=[x], outputs=x_sum)\n",
    "# execute (Python list)\n",
    "ary = [[1, 2, 3], [1, 2, 3]]\n",
    "print('Column sum:', calc_sum(ary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column sum: [ 2.  4.  6.]\n"
     ]
    }
   ],
   "source": [
    "# execute (NumPy array)\n",
    "ary = np.array([[1, 2, 3], [1, 2, 3]], dtype=theano.config.floatX)\n",
    "print('Column sum:', calc_sum(ary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 봤듯이, 3가지 간단한 과정을 통해 Theano를 이용할 수 있다. 변수의 정의, 코드의 compiling, 실행. 다음 예시는 Python과 NumPy의 자료형에서 Theano가 어떻게 작용하는지 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorType(float32, matrix)>\n"
     ]
    }
   ],
   "source": [
    "print(x.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theano는 매우 똑똑한 메모리 운영 시스템을 가지고 있어서 메모리를 재사용하기 때문에 속도가 빠르다. 메모리 공간을 여러 장치에 분산시키고, 각각의 버퍼에 별칭을 준다. 이제, 큰 object(arryas)를 분산해주고 여러 함수가 접근을 읽고 쓸 수 있게 부여해주는 _shared_ 변수에 대해 알아보자. 편집한 후에 이러한 object에 대한 업데이트를 수행할 수 있다. Theano에서 메모리 핸들링에 대해 더 자세히 설명하는 것은 이 책의 범위를 넘는다.\n",
    "\n",
    "그러므로 메모리 운용에 대해 알고 싶다면 다음을 봐라.\n",
    "http://deeplearning.net/software/theano/tutorial/aliasing.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z0: [[ 0.]]\n",
      "z1: [[ 6.]]\n",
      "z2: [[ 12.]]\n",
      "z3: [[ 18.]]\n",
      "z4: [[ 24.]]\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "x = T.fmatrix('x')\n",
    "w = theano.shared(np.asarray([[0.0, 0.0, 0.0]], dtype=theano.config.floatX))\n",
    "z = x.dot(w.T)\n",
    "update = [[w, w + 1.0]]\n",
    "# compile\n",
    "net_input = theano.function(inputs=[x], updates=update, outputs=z)\n",
    "# execute\n",
    "data = np.array([[1, 2, 3]], dtype=theano.config.floatX)\n",
    "for i in range(5):\n",
    "    print('z%d:' % i, net_input(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보다시피 메모리 공유는 매우 간단하다. 앞선 예제에서 _update_ 변수를 통해 array를 각 이터레이션마다 1씩 업데이트 하는 것으로 정의했다. 어떤 object를 업데이트 할 것이고, 어떻게 할지 정해준 후에 _theano.function_ 에 들어있는 _update_ 파라미터에 정보를 전달한다.\n",
    "\n",
    "_givens_ 변수 역시 Theano에서 사용하기 좋다. 이는 컴파일링 하기전에 값을 그래프로 삽입해준다. 이 방법을 사용하면, RAM에서 CPU를 통해 GPU로 이동되는 수를 줄여서 공유변수를 이용하는 학습 알고리즘의 속도를 빠르게 할 수 있다. 우리가 데이터셋을 경사하강 하는 동안 여러 번 iterate 하듯이, 만약 _inputs_ 파라미터를 사용한다면, 데이터는 CPU에서 GPU로 여러 번 전송된다. _givens_를 이용하여 메모리에 데이터셋이 맞다면(fit), GPU에 데이터셋을 유지할 수 있다. 코드는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z: [[ 0.]]\n",
      "z: [[ 6.]]\n",
      "z: [[ 12.]]\n",
      "z: [[ 18.]]\n",
      "z: [[ 24.]]\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "data = np.array([[1, 2, 3]], dtype=theano.config.floatX)\n",
    "x = T.fmatrix('x')\n",
    "w = theano.shared(np.asarray([[0.0, 0.0, 0.0]], dtype=theano.config.floatX))\n",
    "z = x.dot(w.T)\n",
    "update = [[w, w + 1.0]]\n",
    "# compile\n",
    "net_input = theano.function(inputs=[],\n",
    "updates=update,\n",
    "givens={x: data},\n",
    "outputs=z)\n",
    "# execute\n",
    "for i in range(5):\n",
    "    print('z:', net_input())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_givens_의 속성은 파이썬 dictionary과 같다. 여기서, _fmatrix_를 정의할 때, 이름을 설정했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping things up – a linear regression example\n",
    "\n",
    "이제 좀 친숙해졌으니, __Ordinary Least Squares (OLS) __ regression을 해보자.\n",
    "\n",
    "다음과 같이 1차원 toy dataset을 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.asarray([[0.0], [1.0], [2.0], [3.0], [4.0], [5.0],\n",
    "                      [6.0], [7.0], [8.0], [9.0]],dtype=theano.config.floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = np.asarray([[1.0], [1.3], [3.1], [2.0], [5.0], [6.3],\n",
    "                      [6.6], [7.4], [8.0], [9.0]],dtype=theano.config.floatX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형회귀모델의 weight를 학습시키기 위해 training function을 구현해보자.SSE cost function을 이용한다. w0는 bias임을 명심해라. 코드는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "from theano import tensor as T\n",
    "import numpy as np\n",
    "\n",
    "def train_linreg(X_train, y_train, eta, epochs):\n",
    "    \n",
    "    costs = []\n",
    "    # Initialize arrays\n",
    "    eta0 = T.fscalar('eta0')\n",
    "    y = T.fvector(name='y')\n",
    "    X = T.fmatrix(name='X')\n",
    "    w = theano.shared(np.zeros(shape=(X_train.shape[1] + 1),\n",
    "                               dtype=theano.config.floatX), name='w')\n",
    "    \n",
    "    # calculate cost\n",
    "    net_input = T.dot(X, w[1:]) + w[0]\n",
    "    errors = y - net_input\n",
    "    cost = T.sum(T.pow(errors, 2))\n",
    "    \n",
    "    # perform gradient update\n",
    "    gradient = T.grad(cost, wrt=w)\n",
    "    update = [(w, w - eta0 * gradient)]\n",
    "    \n",
    "    # compile model\n",
    "    train = theano.function(inputs=[eta0], outputs=cost, updates=update,\n",
    "                            givens={X: X_train, y: y_train,})\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        costs.append(train(eta))\n",
    "    \n",
    "    return costs, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theano의 매우 좋은 기능은, _grad_ function 이다. _grad_ function은 _wrt_ argument를 사용함으로써 파라미터와 관련한 미분된 식을 자동으로 계산해준다.\n",
    "\n",
    "training function을 구현한 후에, 실제 선형회귀모델을 train해보자. SSE를 살펴보면서 비용 함수가 수렴하는지 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot convert Type TensorType(float32, matrix) (of Variable <TensorType(float32, matrix)>) into Type TensorType(float32, vector). You can try to manually convert <TensorType(float32, matrix)> into a TensorType(float32, vector).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-3f8c5ecc58d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcosts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_linreg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcosts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcosts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-5b041cecc3f5>\u001b[0m in \u001b[0;36mtrain_linreg\u001b[0;34m(X_train, y_train, eta, epochs)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[1;31m# compile model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     train = theano.function(inputs=[eta0], outputs=cost, updates=update,\n\u001b[0;32m---> 26\u001b[0;31m                             givens={X: X_train, y: y_train})\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\theano\\compile\\function.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[1;32m    318\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                    output_keys=output_keys)\n\u001b[0m\u001b[1;32m    321\u001b[0m     \u001b[1;31m# We need to add the flag check_aliased inputs if we have any mutable or\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[1;31m# borrowed used defined inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\theano\\compile\\pfunc.py\u001b[0m in \u001b[0;36mpfunc\u001b[0;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m    440\u001b[0m                                          \u001b[0mrebuild_strict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrebuild_strict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                                          \u001b[0mcopy_inputs_over\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m                                          no_default_updates=no_default_updates)\n\u001b[0m\u001b[1;32m    443\u001b[0m     \u001b[1;31m# extracting the arguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0minput_variables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcloned_extended_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother_stuff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_vars\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\theano\\compile\\pfunc.py\u001b[0m in \u001b[0;36mrebuild_collect_shared\u001b[0;34m(outputs, inputs, replace, updates, rebuild_strict, copy_inputs_over, no_default_updates)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m                 \u001b[0mcloned_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone_v_get_shared_updates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy_inputs_over\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m                 \u001b[0mcloned_outputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcloned_v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOut\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\theano\\compile\\pfunc.py\u001b[0m in \u001b[0;36mclone_v_get_shared_updates\u001b[0;34m(v, copy_inputs_over)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mowner\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclone_d\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mowner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                     \u001b[0mclone_v_get_shared_updates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy_inputs_over\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 clone_d[owner] = owner.clone_with_new_inputs(\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\theano\\compile\\pfunc.py\u001b[0m in \u001b[0;36mclone_v_get_shared_updates\u001b[0;34m(v, copy_inputs_over)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mowner\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclone_d\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mowner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                     \u001b[0mclone_v_get_shared_updates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy_inputs_over\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 clone_d[owner] = owner.clone_with_new_inputs(\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\theano\\compile\\pfunc.py\u001b[0m in \u001b[0;36mclone_v_get_shared_updates\u001b[0;34m(v, copy_inputs_over)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 clone_d[owner] = owner.clone_with_new_inputs(\n\u001b[0;32m---> 95\u001b[0;31m                     [clone_d[i] for i in owner.inputs], strict=rebuild_strict)\n\u001b[0m\u001b[1;32m     96\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mold_o\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_o\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mowner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclone_d\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mowner\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                     \u001b[0mclone_d\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold_o\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_o\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\theano\\gof\\graph.py\u001b[0m in \u001b[0;36mclone_with_new_inputs\u001b[0;34m(self, inputs, strict)\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mstrict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m                     \u001b[1;31m# If compatible, casts new into curr.type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m                     \u001b[0mnew_inputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                     \u001b[0mremake_node\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\theano\\tensor\\type.py\u001b[0m in \u001b[0;36mfilter_variable\u001b[0;34m(self, other, allow_convert)\u001b[0m\n\u001b[1;32m    231\u001b[0m             dict(othertype=other.type,\n\u001b[1;32m    232\u001b[0m                  \u001b[0mother\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m                  self=self))\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mvalue_validity_msg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert Type TensorType(float32, matrix) (of Variable <TensorType(float32, matrix)>) into Type TensorType(float32, vector). You can try to manually convert <TensorType(float32, matrix)> into a TensorType(float32, vector)."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "costs, w = train_linreg(X_train, y_train, eta=0.001, epochs=10)\n",
    "plt.plot(range(1, len(costs)+1), costs)\n",
    "plt.tight_layout()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 번째 epoch 이후 수렴한 것을 알 수 있다.\n",
    "\n",
    "여태까지는 그런대로 잘 됐다. 비용함수를 보면, 특정 데이터셋으로부터 회귀 모델을 만든 것 처럼 보인다. 이제, 새로운 함수를 컴파일해서 예측을 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_linreg(X, w):\n",
    "    Xt = T.matrix(name='X')\n",
    "    net_input = T.dot(Xt, w[1:]) + w[0]\n",
    "    predict = theano.function(inputs=[Xt], givens={w: w}, outputs=net_input)\n",
    "    return predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_train, y_train, marker='s', s=50)\n",
    "plt.plot(range(X_train.shape[0]),Parallelizing Neural Network Training with Theano,\n",
    "         predict_linreg(X_train, w), color='gray',\n",
    "         marker='o', markersize=4, linewidth=3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보다시피 잘 fit 됐다. 간단한 회귀모델을 구현하는 것은 Theano API와 친숙해지기 좋다. 그러나 궁극적인 목표는 Theano의 장점을 끌어내는, 즉 강력한 인공신경망을 구현하는 것이다. Theano를 기반으로 한 필자가 가장 좋아하는 딥러닝 라이브러리를 소개한다. 이는 뉴럴 넷을 최대한 편하게 실험할 수 있게 해준다. 그러나 __Keras__ 라이브러리를 소개하기 전에, 뉴럴 넷의 activation 함수의 다른 선택에 대해 얘기해본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing activation functions for feedforward neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "간단하게 하기 위해서 여태까지는 multilayer feedforward 뉴럴 넷의 맥락에서 sigmoid 액티베이션 함수에 대해 얘기했다. Multilapyer perceptron에서 output layer와 hidden layer  모두 sigmoid를 사용했다. sigmoid 함수는 _logistic function_ 혹은 _negative log-likelihood function_ 이라고 하는게 정확하다. 멀티레이어 뉴럴넷을 향상시키기 위해 sigmoid 외에 대안을 알아보자.\n",
    "\n",
    "기술적으로는, 우리는 미분가능한 액티베이션 함수를 사용할 수 있다. 혹은 Adaline과 같은 선형 activation 함수를 사용할 수도 있다. 그러나 실전에서는 hidden과 output layer에 선형 activation 함수를 이용하는 것은 매우 유용하지 않다. 왜냐면 복잡한 문제를 해결해주는 전형적인 뉴럴 넷에서 비선형성을 표현하고 싶기 때문이다. 선형함수의 합은 결국 선형함수를 얘기하게 된다.\n",
    "\n",
    "로지스틱 액티베이션 함수는 뇌의 뉴런의 개념과 유사하다. 우리는 이것을 뉴런이 활성화 할지 아닐지에 대한 확률로 생각할 수 있다. 그러나 로지스틱 액티베이션 함수는 매우 큰 음수를 input으로 가질 때 문제가 될 수도 있다. 왜냐면 이런 경우에 sigmoid의 아웃풋이 0에 가까워지기 때문이다. 만약 sigmoid 함수가 아웃풋을 0에 가깝게 반환한다면, 뉴럴넷은 매우 느리게 학습할 것이고 학습하는 동안 local minima에 빠질 가능성이 높아진다. 그래서 사람들은 히든레이어에서 __hyperbolic tangent__ 를 사용하는 것을 선호한다. 우선, multi-class classification에서 로지스틱 함수의 개념을 다시 살펴보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic function recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 섹션에서는, sigmoid 함수라고 불리는 로지스틱 함수에 대해 살펴본다. 정확히는 sigmoid 함수의 특별한 케이스이다. Chp3에서 로지스틱 함수를 이용하여 sample x가 어느 class에 속하는지 확률을 모델링하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](figures/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기 스칼라 변수인 z가 net input으로 다음과 같이 정의된다.\n",
    "\n",
    "![Image](figures/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w_0은 bias unit임을 기억해라. 2차원 데이터 포인트 x와 weight coefficients 벡터 w를 갖는 모델을 가정해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fc37b0d55fb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mnet_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "X = np.array([[1, 1.4, 1.5]])\n",
    "w = np.array([0.0, 0.2, 0.4])\n",
    "\n",
    "def net_input(X, w):\n",
    "    z = X.dot(w)\n",
    "    return z\n",
    "\n",
    "def logistic(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def logistic_activation(X, w):\n",
    "    z = net_input(X, w)\n",
    "    return logistic(z)\n",
    "\n",
    "print('P(y=1|x) = %.3f' % logistic_activation(X, w)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "net input을 계산해서 특정 변수 값과 weight 계수를 통해 로지스틱 뉴런을 activate 하는데 사용했다면, 0.707을 얻을 것이다. 이것은 x가 positive class에 속할 가능성이 70.7%라는 것이다. Chp12에서는 멀티 클래스기 때문에 one-hot encoding을 사용하여 다중 로지스틱 액티베이션 단위가 존재하는 아웃풋레이어의 값을 구해냈다. 그러나 다음 코드에서 보다싶이, 다중 로지스틱 액티베이션 단위를 갖는 아웃풋레이어는 의미있고 해석이되는 확률 값을 제공하지 못한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# W : array, shape = [n_output_units, n_hidden_units+1]\n",
    "# Weight matrix for hidden layer -> output layer.\n",
    "# note that first column (A[:][0] = 1) are the bias units\n",
    "W = np.array([[1.1, 1.2, 1.3, 0.5], [0.1, 0.2, 0.4, 0.1], [0.2, 0.5, 2.1, 1.9]])\n",
    "\n",
    "# A : array, shape = [n_hidden+1, n_samples]\n",
    "# Activation of hidden layer.\n",
    "# note that first element (A[0][0] = 1) is the bias unit\n",
    "A = np.array([[1.0], [0.1], [0.3], [0.7]])\n",
    "\n",
    "# Z : array, shape = [n_output_units, n_samples]\n",
    "# Net input of the output layer.\n",
    "Z = W.dot(A)\n",
    "y_probas = logistic(Z)\n",
    "print('Probabilities:\\n', y_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아웃풋에서 볼 수 있듯이, 확률은 특정 샘플이 첫 번째 클래스에 속할 확률이 대략 88%임을 뜻한다. 또한 2,3 번재 클래스에 속할 확률이 각각 58%, 90%임을 뜻한다. 이는 분명 혼란스럽다. 왜냐면 우리가 아는 확률의 합은 100%이기 때문이다. 그러나 이것은 사실 우리가 우리의 모델에서 정확히 class label을 알고싶은 경우에는 문제가 되지 않는다. 하지만 클래스에 속할 확률을 구할 때는 문제가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_class = np.argmax(Z, axis=0)\n",
    "print('predicted class label: %d' % y_class[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나, 특정 상황에서는, 이러한 멀티클래스 예측의 확률값을 주는 것이 매우 유용하다. 다음 장에서는 logistic function을 일반화한 __softmax__ 함수에 대해 알아본다.\n",
    "\n",
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating probabilities in multi-class classification via the softmax function\n",
    "\n",
    "__softmax__ 함수는 로지스틱 함수의 일반화된 형태이다. 소프트맥스에선, net input z를 갖는 특정 샘플 x의가 _i_ 번째 class에 속할 확률을 모든 M 리니어 함수의 합인 분모로 정규화 식을 통해 계산한다.\n",
    "\n",
    "![Image](figures/5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b6265f138aa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0my_probas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Probabilities:\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_probas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Z' is not defined"
     ]
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z))\n",
    "\n",
    "def softmax_activation(X, w):\n",
    "    z = net_input(X, w)\n",
    "    return sigmoid(z)\n",
    "\n",
    "y_probas = softmax(Z)\n",
    "print('Probabilities:\\n', y_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_probas.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보다싶이, 이제 합이 1이 된다. 또한 두 번째 클래스의 확률값이 0에 가까운 것을 알 수 있다. z_1과 max(z)의 차이가 크기 때문이다. 그러나 로지스틱 함수와 같은 label을 얻는 것을 볼 수 있다. 직관적으로 소프트맥스 함수는 멀티클래스 예측에서 의미있는 클래스-멤버 예측을 하기 위한 표준화된 로지스틱 함수라고 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_class = np.argmax(Z, axis=0)\n",
    "print('predicted class label: %d' % y_class[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadening the output spectrum by using a hyperbolic tangent\n",
    "\n",
    "또다른 인공신경망의 히든레이어에서 자주 쓰이는 sigmoid 함수로는 __hyperbolic tangent__가 있다. 이것은 로지스틱 함수의 rescaled된 버전으로 해석될 수도 있다.\n",
    "\n",
    "![Image](figures/new6.png)\n",
    "\n",
    "하이퍼볼릭 탄젠트의 장점은 넓은 아웃풋 스펙트럼과 개구간 (-1,1)에 속한다는 것이다. 이것은 back propagation 알고리즘의 수렴을 증명해준다. 반대로 로지스틱 함수는 개구간 (0,1) 범위에 있는 하나의 output signal을 반환한다. 직관적으로 비교하기 위해 두 함수를 1차원 공간에 그려보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-40552cd8de2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0me_p\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0me_m\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0me_p\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0me_m\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.005\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mlog_act\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtanh_act\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def tanh(z):\n",
    "    e_p = np.exp(z)\n",
    "    e_m = np.exp(-z)\n",
    "    return (e_p - e_m) / (e_p + e_m)\n",
    "\n",
    "z = np.arange(-5, 5, 0.005)\n",
    "log_act = logistic(z)\n",
    "tanh_act = tanh(z)\n",
    "\n",
    "plt.ylim([-1.5, 1.5])\n",
    "plt.xlabel('net input $z$')\n",
    "plt.ylabel('activation $\\phi(z)$')\n",
    "plt.axhline(1, color='black', linestyle='--')\n",
    "plt.axhline(0.5, color='black', linestyle='--')\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.axhline(-1, color='black', linestyle='--')\n",
    "\n",
    "plt.plot(z, tanh_act, linewidth=2, color='black', label='tanh')\n",
    "plt.plot(z, log_act, linewidth=2, color='lightgreen', label='logistic')\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리기 위해 로지스틱과 하이퍼볼릭탄젠트 함수를 자세히 구현했지만, NumPy의 tanh 함수는 다음과 같이 사용할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tanh_act = np.tanh(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한, 로지스틱 함수도 SciPy의 스페셜 모듈을 통해 이용 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "log_act = expit(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 우리는 인공신경망에서 자주 사용되는 여러 액티베이션 함수에대해 알아봤다. 다음 사진을 보며 이 섹션을 마무리한다.\n",
    "\n",
    "![Image](figures/7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------\n",
    "\n",
    "## Training neural networks efficiently using Keras\n",
    "\n",
    "이번 섹션에서는, 뉴럴 넷의 학습을 촉진하기 위해 개발된 라이브러리중 가장 최근 것인 Keras에 대해 살펴본다. 2015년도 초기에 시작됐다. Theano를 기반으로 만들어진 라이브러리로, GPU가 뉴럴넷의 학습을 가속화시키는데 활용할 수 있다. 매우 직관적인 API라는 것이 특징이다. 불과 몇 줄의 코드로 뉴럴 넷을 구현할 수 있다. Theano가 깔려 있다면, Keras를 다음과 같이 설치한다.\n",
    "\n",
    "__pip install Keras__\n",
    "\n",
    "MNIST 데이터를 다운 받아서, 다음과 같이 실행해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 60000, columns: 784\n",
      "Rows: 10000, columns: 784\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,'%s-labels.idx1-ubyte'% kind)\n",
    "    images_path = os.path.join(path,'%s-images.idx3-ubyte'% kind)\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II',lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, dtype=np.uint8)\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784)\n",
    "    return images, labels\n",
    "\n",
    "X_train, y_train = load_mnist('C:/Users/Administrator/mnist/', kind='train')\n",
    "print('Rows: %d, columns: %d' % (X_train.shape[0], X_train.shape[1]))\n",
    "X_test, y_test = load_mnist('C:/Users/Administrator/mnist/', kind='t10k')\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "당신의 GPU를 사용하여 실행해보고 싶다면, 다음과 같이 command 명령어를 통해 실행해봐라.\n",
    "\n",
    "__THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python mnist_keras_mlp.py__\n",
    "\n",
    "이제 학습시키기 위해 MNIST 이미지 배열을 32-bit 형식으로 변환해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "theano.config.floatX = 'float32'\n",
    "X_train = X_train.astype(theano.config.floatX)\n",
    "X_test = X_test.astype(theano.config.floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-fae7f5602ec9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'First 3 labels: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unknown backend: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_BACKEND\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctc\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mctc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "print('First 3 labels: ', y_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np_utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b8145380cb92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_train_ohe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nFirst 3 labels (one-hot):\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_ohe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np_utils' is not defined"
     ]
    }
   ],
   "source": [
    "y_train_ohe = np_utils.to_categorical(y_train)\n",
    "print('\\nFirst 3 labels (one-hot):\\n', y_train_ohe[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 뉴럴넷을 구현해보자. Chp12와 같은 구조로 실행할 것이다. 그러나 히든레이어의 logistic 유닛을 하이퍼볼탄젠트 액티베이션 함수로 바꾸고, 아웃풋레이어의 로지스틱 함수를 소프트맥스로 바꿀 것이다. 또한 하나의 히든레이어를 추가한다. Keras는 이러한 작업들을 매우 간단하게 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=X_train.shape[1], output_dim=50,\n",
    "                init='uniform', activation='tanh'))\n",
    "\n",
    "\n",
    "model.add(Dense(input_dim=50, output_dim=50, init='uniform',\n",
    "                activation='tanh'))\n",
    "\n",
    "model.add(Dense(input_dim=50, output_dim=y_train_ohe.shape[1],\n",
    "                init='uniform', activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-7, momentum=.9)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 _Sequential_ 클래스를 이용하여 feedforward 신경망을 구현할 새로운 모델을 만든다. 그러고서 많은 레이어를 추가한다. 그러나 우리는 처음에 input 레이어를 추가했으므로, input_dim 는 트레이닝 셋의 feature의 수와 같은지 확인해줘야한다. 또한 두 연속적인 레이어의 아웃풋 unit의 수와 input unit가 같은지 봐줘야한다. 예시에서는 두개의 히든레이어에 50개의 히든 유닛과 1개의 bias 유닛을 각각 넣어줬다. Keras에서 완벽히 연결된 네트워크에서는 bias unit은 0으로 초기화된다. 이것은 bias unit을 1로 초기화했던 MLP를 구현할 때와 비교된다. 1로 하는게 일반적이다.\n",
    "\n",
    "끝내, 아웃풋 레이어의 유닛의 수는 특정 클래스 label의 수와 같아져야만 한다. one-hot encode된 클래스 label의 열의 합이다. 모델을 컴파일하기 전에, optimizer를 정의해야만 한다. 지난 예시에서 SGD를 사용하였으니 이번에도 그렇게 해보자. 각각의 epoch마다 학습률을 적용하기 위해 weight decay constant를 정의할 수도 있다. _categorical_crossentropy_ 를 통해 비용함수를 정의한다. 교차 엔트로피는 단순히 로지스틱 회귀에서 비용함수 위한 테크니컬한 용어이며 __범주형 cross-entropy__는 소프트맥스를 통해 멀티 클래스 예측을 하기 위한 일반화된 것이다. 모델을 컴파일한 후에, fit 메소드를 이용하여 학습을 할 수 있다. 여기서는, 300개의 트레이닝 샘플의 batch 사이즈를 이용하는 mini-batch stochastic gradient를 이용한다. MLP를 50번 반복하여 학습시키며, verbose=1로 세팅 함으로써 비용 함수의 최적화를 따라가본다. _validation_split_ 파라미터는 특히 능숙한데, 이것은 각각의 에폭 후에 검증을 위해 10%의 학습데이터를 반전시킨다. 따라서 학습동안 오버피팅 되는 경우를 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-4d048e1d5003>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit(X_train, y_train_ohe, nb_epoch=50, batch_size=300, verbose=1,\n\u001b[0m\u001b[1;32m      2\u001b[0m           validation_split=0.1, show_accuracy=True)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train_ohe, nb_epoch=50, batch_size=300, verbose=1,\n",
    "          validation_split=0.1, show_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습시키는 동안 비용함수의 값을 출력하는 것은 매우 유용하다. 비용이 갑자기 줄어드는 구간을 찾아내거나, 찾지 못한다면 하이퍼파라미터를 tune하기위해 알고리즘을 종료할 것이다.\n",
    "\n",
    "클래스 레이블을 예측하기 위해, 우리는 _predict_classes_ 메소드를 사용하였고, 이것은 class label을 정수로 정확히 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-850692975d98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_train_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'First 3 predictions: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "y_train_pred = model.predict_classes(X_train, verbose=0)\n",
    "print('First 3 predictions: ', y_train_pred[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_acc = np.sum(y_train == y_train_pred, axis=0) / X_train.shape[0]\n",
    "print('Training accuracy: %.2f%%' % (train_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = model.predict_classes(X_test, verbose=0)\n",
    "test_acc = np.sum(y_test == y_test_pred, axis=0) / X_test.shape[0]\n",
    "print('Test accuracy: %.2f%%' % (test_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파라미터 최적화 튜닝하지 않은 아주 간단한 뉴럴넷을 보았다. Kears에 대해 더 알고 싶다면 다음을 참조해라.\n",
    "\n",
    "Although Keras is great library for implementing and experimenting with neural networks, there are many other Theano wrapper libraries that are worth mentioning. A prominent example is Pylearn2 (http://deeplearning.net/software/pylearn2/), which has been developed in the LISA lab in Montreal. Also, Lasagne (https://github.com/Lasagne/Lasagne) may be of interest to you if you prefer a more minimalistic but extensible library, that offers more control over the underlying Theano code."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
