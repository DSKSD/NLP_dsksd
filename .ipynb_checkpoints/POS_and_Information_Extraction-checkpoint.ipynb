{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Noisy Channel Model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example__: <br>\n",
    "* Input: 쓰여진 글(Written) (X)\n",
    "* Encoder: input을 혼동(X->Y)\n",
    "* Output: 발화된 말(Y)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "__More examples__: <br>\n",
    "* 문법적 실수\n",
    "* English to bitmaps(characters)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "__P(X,Y) = P(X)P(Y|X)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The POS task "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 통계적으로 토큰을 문장에서의 역할을 찾아내는 것. (명사, 동사..)\n",
    "\n",
    "<hr>\n",
    "* Open class: 언제든 새 단어를 추가할 수 있음.(nouns, non-modal verbs, adjectives, adverbs)\n",
    "<br>\n",
    "<br>\n",
    "* Closed class: 새로운 인스턴스 추가가 어렵다. (prepositions, modal verbs, conjunctions, particels, determiners, pronouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penn Treebank tagset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Observations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ambiguity(모호함) 동음이의어!\n",
    "  - count(noun) vs. count(verb)\n",
    "  - 11% of all types but 40% of all tokens in the Brown corpus are ambiguous) 이러한 모호한 단어들은 빈도수가 높은 경향이 있다\n",
    "<br>\n",
    "* French pronunciation: 프랑스어는 pos에 따라 강세가 달라 발음이 다르게 발음 됌.\n",
    "<br>\n",
    "* Three main technique:\n",
    "  - rule-based\n",
    "  - machine learning(ex. CRF, maximum entropy, HMM)\n",
    "  - transformation-based <br>\n",
    "<br>\n",
    "머신러닝 기반과 트랜스포매이션 기반이 룰베이스드에 비해 훨씬 강력하다. 도메인이나 다른 언어로 확장하기 쉽기 때문.<br>\n",
    "<br>\n",
    "\n",
    "* Useful to parsing, translation, text to speech, word sense disambiguation, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources of Information "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음에 올 단어의 POS를 정하기 위해 전 단어들의 POS를 본다?!\n",
    "\n",
    "* Knowledge about individual words\n",
    "  - lexical information\n",
    "  - spelling (-or)\n",
    "  - capitalization (IBM) # 근데 이런건 영어니까 가능한듯..<br>\n",
    "<br>\n",
    "* Knowledge about neighboring words(한 단어 전, 두 단어 전 등등..!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Baseline\n",
    "  - tag each word with its most likely tag\n",
    "  - tag each OOV(사전에 없는) word as a noun\n",
    "  - around 90% <사전에 없는 단어들을 다 명사로 태깅했더니 90퍼센트의 정확도가 나온다? 이거 문제?) 그래서 진짜 쓸만하려면 90퍼센트보다 훨씬 높아야 한다!??<br>\n",
    "<br>\n",
    "* Current accuracy\n",
    "  - around 97% for English\n",
    "  - compared to 98% human performance (2퍼센트 모자란건 사람마다 의견이 다른 부분이 존재해서..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-based POS tagging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use dictionary or finite-state transducers to find all possible parts of speech (사전을 사용하거나 가능한 모든 상태(규칙)을 찾아내서 명시한다..)\n",
    "* Use disambiguation rules(명확한 룰)\n",
    "* Hundreds of constraints can be designed manually\n",
    "<hr>\n",
    "ex> 3인칭 대명사는 1인칭 대명사 뒤에 나올 수 없다.?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초기의 상태가 존재하고 어떠한 상태(State)들은 추이행렬에 의해 전이 되며(마코프 행렬) 그 상태마다 어떠한 관측값(Observation)이 출력될 확률이 존재하는 2중 모델!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sequence of random variables that aren't independent\n",
    "* Examples\n",
    "  - weather reports\n",
    "  - text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Limeted horizon:\n",
    "  - $ P(X_{t+1} = s_k | X_1,...X_t) = P(X_{t+1} = s_k | X_t) $\n",
    "* Time invariant(stationary)\n",
    "  = $ P(X_2 = s_k|X_1) $\n",
    "* Definition: in terms of a transition matrix A and initial state probabilities $\\pi$ (상태 추이행렬과 초기 상태 확률)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visible MM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ P(X_1,...X_T) = P(X_1)P(X_2|X_1)P(X_3|X_1,X_2)...P(X_T|X_1,...X_{T-1})$\n",
    "$  Bigram    = P(X_1)P(X_2|X_1)P(X_3|X_2)...P(X_T|X_{T-1}) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden MM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visible MM보다 좀 더 언어 모델링에 적합<br>\n",
    "관측이 불가능한 상태를 관측이 가능한 상태로 추정하는 이중 확률처리 모델..??\n",
    "<br>\n",
    "* Motivation\n",
    "  - Observing a sequence of symbols\n",
    "  - The sequence of states that led to the generation of the symbols is hidden (심볼은 실제 단어, 시퀀스 state는 그 단어의 POS)\n",
    "<br>\n",
    "* Definition\n",
    "  - Q = sequence of states\n",
    "  - O = sequence of observations, drawn from a vocabulary\n",
    "  - $q_0,q_f$ = special (start, final) states\n",
    "  - A = state transition probabilities\n",
    "  - B = symbol emission(배출?) probabilities(그 state(POS)가 주어졌을 때, 그 symbol이 나올 확률?)\n",
    "  - $\\pi$ = initial state probabilities\n",
    "  - $\\mu$ = (A,B,$\\pi$) = complete probabilistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Can be used to model state sequences and observation sequences\n",
    "* Example:\n",
    "  - P(s,w) = $\\pi_iP(s_i|s_{i-1})P(w_i|s_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Algorithm (생성 모델) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pick start state from $\\pi$\n",
    "* For t = 1..T\n",
    "  - Move to another state based on A\n",
    "  - Emit an observation based on B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Parameters of the Model(example) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initial\n",
    "  - P(A|start) = 1.0, P(B|start) = 0.0\n",
    "* Transition\n",
    "  - P(A|A) = 0.8, P(A|B) = 0.6, P(B|A) = 0.2, P(B|B) = 0.4\n",
    "* Emission\n",
    "  - P(x|A) = 0.7, P(y|A) = 0.2, P(z|A) = 0.1\n",
    "  - P(x|B) = 0.3, P(y|B) = 0.5, P(z|B) = 0.2\n",
    "<hr>\n",
    "* Starting in state A, P(yz) = ?\n",
    "* Possible sequences of states:\n",
    "  - AA\n",
    "  - AB\n",
    "  - BA\n",
    "  - BB\n",
    "* P(yz) = P(yz|AA) + P(yz|AB) + P(yz|BA) + P(yz|BB) = \n",
    "0.8x0.2x0.8x0.1 + 0.8x0.2x0.2x0.2 + 0.2x0.5x0.4x0.2 + 0.2x0.5x0.6x0.1 = 0.0332 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## States and Transitions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The states encode the most recent history\n",
    "* The transitions encode likely sequences of states\n",
    "  - e.g., Adj-Noun or Noun-Verb\n",
    "  - or perhaps Art-Adj-Noun\n",
    "* Use MLE to estimate the transition probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emissions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Estimating the emission probabilities\n",
    "  - Harder than transition probabilities\n",
    "  - There may be novel uses of Word/POS combinations\n",
    "* Suggestions\n",
    "  - It is possible to use standard smoothing\n",
    "  - As well as heuristics(e.g., based on the spelling of the words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence of Observations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The observer can only see the emitted symbols\n",
    "* Observation likelihood\n",
    "  - Given the observation sequence S and the model $\\mu = (A,B,\\pi)$, what is the probability P(S|$\\pi$) that the sequence was generated by that model.\n",
    "* Being able to compute the probability of the observations sequence turns the HMM into a language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks with HMM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tasks\n",
    "  - Given $\\mu = (A,B,\\pi), find P(O|\\mu)$\n",
    "  - Given O, $\\mu$, what is ($X_1,...X_{T+1}$)\n",
    "  - Given O and a space of all possible $\\mu$, find model that best describes the observations\n",
    "<br>\n",
    "<br>\n",
    "* __Decoding - most liekly sequence__(tag each token with a label)\n",
    "* Observation likelihood(classify sequences)\n",
    "* Learning(train models to fit empirical data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 단어의 시퀀스가 주어질 때, 태그의 시퀀스를 추론!(Decoding)\n",
    "  - $t^* = argmax_tP(t|w)$\n",
    "* Given the model $\\mu$, it is possible to compute P(t|w) for all values of t\n",
    "* 실제로는 너무 많은 조합이 존재할 수 있다..!\n",
    "* __Beam search__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viterbi Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Find the best path up to observation i and state s \n",
    "* Characteristics\n",
    "  - Uses dynamic programming\n",
    "  - Memoization\n",
    "  - Backpointer(어떤 포인트에서 뒤 돌아볼 수 있는?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Given multiple HMMs\n",
    "  - e.g., for different languages\n",
    "  - Which one is the most likely to have generated the observation sequence<br>\n",
    "* Naive solution\n",
    "  - try all possible state sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dynamic programming method\n",
    "  - Computing a forward trellis that encodes all possible state paths\n",
    "  - Based on the Markov assumption that the probability of being in any state at a given time point only depends on the probabilities of being in all states at the previous time point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## HMM Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Supervised\n",
    "  - Training sequences are labeled\n",
    "* Unsupervised\n",
    "  - Training sequences are unlabeled\n",
    "  - Known number of states ( start와 end를 포함한 POS 갯수)\n",
    "* Semi-supervised\n",
    "  - Some training sequences are labeled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised HMM Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Estimate the static transition probabilities using MLE\n",
    "<br>\n",
    "<br>\n",
    "$$ a_{ij} = \\frac{Count(q_t = s_i, q_{t+q} = s_j)}{Count(q_t=s_i)}$$\n",
    "<br>\n",
    "<br>\n",
    "* Estimate the observation probabilities using MLE\n",
    "<br>\n",
    "<br>\n",
    "$$ b_j(k) = \\frac{Count(q_i = s_j, o_i = v_k)}{Count(q_i=s_j)}$$\n",
    "<br>\n",
    "* Use smoothing(unseen data에 확률 재할당)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised HMM Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Given : Observation sequences\n",
    "* Goal : build the HMM\n",
    "* Use EM(Expectation Maximization) methods\n",
    "  - forward-backward(Baum-Welch) algorithm\n",
    "  - Baum-Welch finds an approximate solution for P(O|$\\mu$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline of Baum-Welch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Algorithm\n",
    "  - Randomly set the parameters of the HMM\n",
    "  - Until the parameters converge repeat:\n",
    "    - E step - determine the probability of the various state sequences for generating the observations\n",
    "    - M step - reestimate the parameters based on these probabilities\n",
    "* Notes\n",
    "  - the algorithm guarantees that at each iteration the likelihood of the data P(O|$\\mu$) increases\n",
    "  - it can be stopped at any poing the give a partial solution\n",
    "  - it converges to a local maximum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM Tagging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* T = argmaxP(T|W)\n",
    "* By Bayes' tehorem\n",
    "  - P(T|W) = P(T)P(W|T)/P(W)\n",
    "* Thus we are attempting to choose the sequence of tags that maximizes the right hand side of the equation\n",
    "  - P(W) can be innored\n",
    "  - P(T) is called the prior, P(W|T) is called the likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Taggers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data set\n",
    "  - Training set\n",
    "  - Development set\n",
    "  - Test set\n",
    "* Tagging accuracy\n",
    "  - how many tags right\n",
    "* Results\n",
    "  - Accuracy around 97% on PTB trained on 800,000 words\n",
    "  - (50~85% on unknown words; 50% for trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- People\n",
    "- Locations\n",
    "- Organizations\n",
    "  - Teams\n",
    "  - Newspapers\n",
    "  - Companies\n",
    "- Geo-political entities\n",
    "<br>\n",
    "<br>\n",
    "* Ambiuity:\n",
    "  - London은 사람이 될 수도, 도시가 될 수도, 국가가 될 수도.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Times and Events "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Times\n",
    "  - Absolute expressions\n",
    "  - Relative expressions(e.g., \"late night\")\n",
    "* Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Labeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Many NLP problems can be cast as sequence labeling problems\n",
    "  - POS - Part of speech tagging\n",
    "  - NER - named entitiy recognition\n",
    "  - SRL - sementic role labeling\n",
    "* Input : Sequence $w_1w_2w_3$\n",
    "* Ouptput : Labeled words\n",
    "* Classification methods\n",
    "  - Can use the categories of the previous tokens as features in classifying the next one\n",
    "  - Direction matters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition(NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Segmentation\n",
    "  - Which words belong to a named entitiy?\n",
    "* Classification\n",
    "  - What type of named entity is it?\n",
    "  - Use gazetteers, spelling, adjacent words, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
