{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class DQN:\n",
    "\n",
    "    # 다음에 취할 액션을 DQN 을 이용해 결정할 시기를 결정합니다.\n",
    "    # get_action 함수 참고\n",
    "    INITIAL_EPSILON = 1.0\n",
    "    FINAL_EPSILON = 0.001\n",
    "    EXPLORE = 1000.\n",
    "    # 학습 데이터를 어느정도 쌓은 후, 일정 시간 이후에 학습을 시작하도록 합니다.\n",
    "    OBSERVE = 100.\n",
    "    # 학습에 사용할 플레이결과를 얼마나 많이 저장해서 사용할지를 정합니다.\n",
    "    # (플레이결과 = 게임판의 상태 + 취한 액션 + 리워드 + 종료여부)\n",
    "    REPLAY_MEMORY = 10000\n",
    "    # 학습시 사용/계산할 상태값(정확히는 replay memory)의 갯수를 정합니다.\n",
    "    BATCH_SIZE = 32\n",
    "    # 과거의 상태에 대한 가중치를 줄이는 역할을 합니다.\n",
    "    GAMMA = 0.7\n",
    "    \n",
    "    #NOISE = 0.2 # 0.2보다 작으면 0으로 만들어 distortion\n",
    "    \n",
    "    \n",
    "    def __init__(self, n_action, n_width, n_height, state):\n",
    "        self.n_action = n_action\n",
    "        self.n_width = n_width\n",
    "        self.n_height = n_height\n",
    "\n",
    "        self.time_step = 0\n",
    "        self.epsilon = self.INITIAL_EPSILON\n",
    "        # 게임의 상태. 게임판의 상태를 말합니다.\n",
    "        # 학습으로 계산할 상태는 현재 게임판과, 과거 세 번의 게임판, 총 네 가지의 상태를 사용합니다.\n",
    "        self.state_t = np.stack((state, state, state, state), axis=1)[0]\n",
    "        # 게임 플레이결과를 저장할 메모리\n",
    "        self.memory = deque()\n",
    "\n",
    "        # 게임의 상태를 입력받을 변수\n",
    "        # [게임 상태의 갯수(현재+과거+과거..), 각 시점의 게임의 상태(게임판의 크기)]\n",
    "        self.input_state = tf.placeholder(tf.float32, [None, len(self.state_t), self.n_width * self.n_height])\n",
    "        # 각각의 상태를 만들어낸 액션의 값들입니다.\n",
    "        self.input_action = tf.placeholder(tf.float32, [None, self.n_action])\n",
    "        # DQN 의 가장 핵심적인 값이며 Q_action 을 계산하는데 사용할 값 입니다. train 함수를 참고하세요.\n",
    "        self.input_Y = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "        self.global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "        self.rewards = tf.placeholder(tf.float32, [None])\n",
    "        tf.summary.scalar('avg.reward', tf.reduce_mean(self.rewards))\n",
    "\n",
    "        self.Q_value, self.train_op = self.build_model()\n",
    "\n",
    "        self.saver, self.session = self.init_session()\n",
    "        self.writer = tf.summary.FileWriter('logs', self.session.graph)\n",
    "        self.summary = tf.summary.merge_all()\n",
    "\n",
    "    def init_session(self):\n",
    "        saver = tf.train.Saver()\n",
    "        session = tf.InteractiveSession()\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state('model')\n",
    "        if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "            print(\"다음 파일에서 모델을 읽는 중 입니다..\", ckpt.model_checkpoint_path)\n",
    "            saver.restore(session, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            print (\"새로운 모델을 생성하는 중 입니다.\")\n",
    "            session.run(tf.global_variables_initializer())\n",
    "\n",
    "        return saver, session\n",
    "\n",
    "    def write_logs(self, reward):\n",
    "        if self.time_step % 100 == 0:\n",
    "            summary = self.summary.eval(feed_dict={self.rewards: reward})\n",
    "            self.writer.add_summary(summary, self.global_step.eval())\n",
    "\n",
    "        if self.time_step % 2500 == 0:\n",
    "            self.saver.save(self.session, 'model/dqn.ckpt', global_step=self.time_step)\n",
    "\n",
    "    def build_model(self):\n",
    "        # 계산 속도와 편의성을 위해 CNN 을 사용하지 않고, input_state 값을 flat 하게 만들어 계산합니다.\n",
    "        n_input = len(self.state_t) * self.n_width * self.n_height\n",
    "        state = tf.reshape(self.input_state, [-1, n_input])\n",
    "\n",
    "        W1 = tf.Variable(tf.truncated_normal([n_input, 128], stddev=0.01))\n",
    "        b1 = tf.Variable(tf.constant(0.01, shape=[128]))\n",
    "        L1 = tf.nn.relu(tf.matmul(state, W1) + b1)\n",
    "\n",
    "        W2 = tf.Variable(tf.truncated_normal([128, 256], stddev=0.01))\n",
    "        b2 = tf.Variable(tf.constant(0.01, shape=[256]))\n",
    "        L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "        W3 = tf.Variable(tf.truncated_normal([256, self.n_action], stddev=0.01))\n",
    "        b3 = tf.Variable(tf.constant(0.01, shape=[self.n_action]))\n",
    "        Q_value = tf.matmul(L2, W3) + b3\n",
    "\n",
    "        # DQN 의 손실 함수를 구성하는 부분입니다. 다음 수식을 참고하세요.\n",
    "        # Perform a gradient descent step on (y_j-Q(ð_j,a_j;θ))^2\n",
    "        Q_action = tf.reduce_sum(tf.mul(Q_value, self.input_action), axis=1)\n",
    "        cost = tf.reduce_mean(tf.square(self.input_Y - Q_action))\n",
    "        train_op = tf.train.AdamOptimizer(1e-6).minimize(cost, global_step=self.global_step)\n",
    "\n",
    "        return Q_value, train_op\n",
    "\n",
    "    def train(self):\n",
    "        # 게임 플레이를 저장한 메모리에서 배치 사이즈만큼을 샘플링하여 가져옵니다.\n",
    "        minibatch = random.sample(self.memory, self.BATCH_SIZE)\n",
    "\n",
    "        state = [data[0] for data in minibatch]\n",
    "        action = [data[1] for data in minibatch]\n",
    "        reward = [data[2] for data in minibatch]\n",
    "        next_state = [data[3] for data in minibatch]\n",
    "\n",
    "        Y = []\n",
    "        Q_value = self.Q_value.eval(feed_dict={self.input_state: next_state}) # target Q_value를 계산하기 위한 값! (greedy 방식)\n",
    "\n",
    "        # DQN 의 손실 함수에 사용할 핵심적인 값을 계산하는 부분입니다. 다음 수식을 참고하세요.\n",
    "        # if episode is terminates at step j+1 then r_j\n",
    "        # otherwise r_j + γ*max_a'Q(ð_(j+1),a';θ')\n",
    "        for i in range(0, self.BATCH_SIZE):\n",
    "            if minibatch[i][4]:\n",
    "                Y.append(reward[i])\n",
    "            else:\n",
    "                Y.append(reward[i] + self.GAMMA * np.max(Q_value[i])) # greedy 방식으로 target\n",
    "\n",
    "        self.train_op.run(feed_dict={ # state, action, Y를 이용해 Q-value Estimator를 훈련시킨다.\n",
    "            self.input_Y: Y,\n",
    "            self.input_action: action,\n",
    "            self.input_state: state\n",
    "        })\n",
    "\n",
    "        self.write_logs(reward)\n",
    "\n",
    "    def step(self, state, action, reward, terminal):\n",
    "        # 학습데이터로 현재의 상태만이 아닌, 과거의 상태까지 고려하여 계산하도록 하였고,\n",
    "        # 이 모델에서는 과거 3번 + 현재 = 총 4번의 상태를 계산하도록 하였으며,\n",
    "        # 새로운 상태가 들어왔을 때, 가장 오래된 상태를 제거하고 새로운 상태를 넣습니다.\n",
    "        next_state = np.append(self.state_t[1:, :], state, axis=0)\n",
    "        # 플레이결과, 즉, 액션으로 얻어진 상태와 보상등을 메모리에 저장합니다.\n",
    "        self.memory.append((self.state_t, action, reward, next_state, terminal))\n",
    "\n",
    "        # 저장할 플레이결과의 갯수를 제한합니다.\n",
    "        if len(self.memory) > self.REPLAY_MEMORY:\n",
    "            self.memory.popleft() # 오래된 메모리를 pop\n",
    "\n",
    "        # 일정시간 이상 반복이 이루어진 이후에(데이터가 쌓인 이후에) 학습을 시작합니다.\n",
    "        if self.time_step > self.OBSERVE:\n",
    "            self.train()\n",
    "\n",
    "        self.state_t = next_state\n",
    "        self.time_step += 1\n",
    "\n",
    "    def get_action(self, train=False):\n",
    "        # action 과 Q_value 는 one-hot 벡터를 이용합니다.\n",
    "        action = np.zeros(self.n_action)\n",
    "\n",
    "        # 학습 초기에는 액션을 랜덤한 값으로 결정합니다.\n",
    "        # 이후 학습을 진행하면서 점진적으로 더 많은 결정을 DQN 이 하도록 합니다.\n",
    "        if train and random.random() <= self.epsilon:\n",
    "            index = random.randrange(self.n_action)\n",
    "        else:\n",
    "            Q_value = self.Q_value.eval(feed_dict={self.input_state: [self.state_t]})[0]\n",
    "            index = np.argmax(Q_value)\n",
    "\n",
    "        action[index] = 1\n",
    "\n",
    "        # 학습이 일정시간 이상 지났을 때부터 입실론 값을 점진적으로 줄이며,\n",
    "        # 얼마나 단계적으로, 또 얼마나 많이 액션값을 DQN 에 맡길지를 결정하기 위한 로직입니다.\n",
    "        if self.epsilon > self.FINAL_EPSILON and self.time_step > self.OBSERVE:\n",
    "            self.epsilon -= (self.INITIAL_EPSILON - self.FINAL_EPSILON) / self.EXPLORE\n",
    "\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
